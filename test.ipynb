{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uuid': '9c020153-49fb-4acf-8b92-babcac58b839', 'class': 'signature', 'confidence': 0.9773558378219604, 'bbox': [836, 2258, 1464, 2406], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '1434690e-b3c0-45cb-8271-42c24b38b17b', 'class': 'signature', 'confidence': 0.8746067881584167, 'bbox': [1552, 2258, 2169, 2402], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'faa53783-f4b0-428c-97ee-bc5a7a6e87c2', 'class': 'boxInput', 'confidence': 0.766650915145874, 'bbox': [405, 1588, 2234, 1658], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '2a90fbed-011b-4ae9-9a8e-5597da7a234e', 'class': 'boxInput', 'confidence': 0.7511256337165833, 'bbox': [1691, 218, 2225, 290], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '2146398f-1f97-490b-a2d0-020c0ae0b922', 'class': 'boxInput', 'confidence': 0.7268276214599609, 'bbox': [412, 889, 890, 961], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '0b78e7cd-e74c-48ae-8eef-2f229ef7b8bc', 'class': 'boxInput', 'confidence': 0.7190621495246887, 'bbox': [390, 816, 1439, 883], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'b8a3674d-db51-470c-8732-6064645fcc88', 'class': 'boxInput', 'confidence': 0.7116532921791077, 'bbox': [1730, 898, 2256, 969], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '02be4b74-21ba-4fc4-9370-e1233fa69ad9', 'class': 'boxInput', 'confidence': 0.7053065299987793, 'bbox': [411, 1165, 1180, 1232], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'c12a54a0-1406-493a-8ca7-5b71b959019a', 'class': 'checkBox', 'confidence': 0.6866014003753662, 'bbox': [754, 567, 849, 643], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'ca6476ac-3016-47d4-b805-1d1d37989f0b', 'class': 'boxInput', 'confidence': 0.6551302671432495, 'bbox': [1588, 291, 2230, 358], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'cc4ba0c5-0f8e-440c-8732-b48d15cdf2d4', 'class': 'boxInput', 'confidence': 0.6492753028869629, 'bbox': [412, 967, 2245, 1034], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'c4046e2a-f198-4351-9d9a-7586c1803cd9', 'class': 'boxInput', 'confidence': 0.6355304718017578, 'bbox': [405, 1035, 2248, 1099], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'fd9bfad6-a1cd-4689-a70f-80cb351cb998', 'class': 'boxInput', 'confidence': 0.6334598660469055, 'bbox': [412, 1521, 1128, 1581], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '9253f838-80ea-40aa-9815-d3b39ed4bb9c', 'class': 'boxInput', 'confidence': 0.6323598027229309, 'bbox': [413, 1450, 2237, 1521], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '34d4719f-8b3f-4f5d-8a45-6f4a3e6a8dca', 'class': 'boxInput', 'confidence': 0.6248471736907959, 'bbox': [405, 1096, 2237, 1167], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'a976c8e0-e77e-4cce-a7d9-d0f8007c20fb', 'class': 'signature', 'confidence': 0.5402045249938965, 'bbox': [109, 2255, 738, 2406], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '53453e65-e09b-4d8f-8d0b-7b3e515ab768', 'class': 'lineInput', 'confidence': 0.5089089870452881, 'bbox': [1084, 1870, 1390, 1912], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'e7536d1d-3b2b-468e-8673-3c518d7be58b', 'class': 'lineInput', 'confidence': 0.49924802780151367, 'bbox': [1219, 2979, 1653, 3043], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '60201027-2ae9-40c1-8ccf-86f9a0239c18', 'class': 'lineInput', 'confidence': 0.4755920171737671, 'bbox': [786, 2739, 1165, 2795], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'ea73d990-dacd-4dba-87ea-93a7cc678149', 'class': 'boxInput', 'confidence': 0.425040066242218, 'bbox': [413, 1384, 1422, 1450], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '9a7311fc-28e1-49b2-9437-b2fd98aeba83', 'class': 'lineInput', 'confidence': 0.36865338683128357, 'bbox': [1427, 2662, 2188, 2720], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'ce03c8fe-233d-408c-842b-7b7e2119f5e0', 'class': 'lineInput', 'confidence': 0.3612613379955292, 'bbox': [1429, 2733, 2164, 2797], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '5849f1bc-d0aa-4888-9525-2a034aeae582', 'class': 'lineInput', 'confidence': 0.3450714349746704, 'bbox': [103, 2808, 2143, 2881], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'e7600f3f-34d3-4dd1-a6a2-54847e2921c9', 'class': 'lineInput', 'confidence': 0.3286809027194977, 'bbox': [744, 1664, 2166, 1730], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'cd927bcf-ed62-4975-a8e3-df83f3a5169b', 'class': 'lineInput', 'confidence': 0.3165976405143738, 'bbox': [204, 3028, 731, 3100], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '8cc181fc-dac3-4d4e-8726-7f1eefcec2ce', 'class': 'lineInput', 'confidence': 0.3116876780986786, 'bbox': [1111, 2908, 1655, 2966], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'c1157b3c-a9ee-466b-9c2c-a2551a889531', 'class': 'lineInput', 'confidence': 0.2956661581993103, 'bbox': [1724, 403, 2235, 465], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': '255c84ce-755b-4483-a034-000f509b48d0', 'class': 'lineInput', 'confidence': 0.28276658058166504, 'bbox': [942, 387, 1394, 454], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'id': 'dfea8d0a-81f2-4764-a9fa-5ed221193919', 'class': 'checkBox', 'confidence': 1, 'bbox': [416, 567, 513, 638], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'id': '73e0e83e-fcb8-490a-880c-f20f46f6ddf0', 'class': 'checkBox', 'confidence': 1, 'bbox': [109, 562, 201, 637], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'id': '43a1e631-aee2-4b67-8cd1-32f9d497cd9f', 'class': 'EmptyInput', 'confidence': 1, 'bbox': [116, 450, 2147, 517], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'id': '8963bc8a-0ef9-4087-80d0-c42883c8dc95', 'class': 'boxInput', 'confidence': 1, 'bbox': [414, 1308, 2241, 1384], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'id': '906c84a7-55d5-4ff7-a918-a0f260491738', 'class': 'EmptyInput', 'confidence': 1, 'bbox': [165, 2954, 656, 3029], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'id': '9ee820c0-d529-4250-b551-d8ac28f9757c', 'class': 'EmptyInput', 'confidence': 1, 'bbox': [415, 2654, 1162, 2722], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'id': '4755fd1b-70ab-4e11-9349-fa5ee0c30a15', 'class': 'EmptyInput', 'confidence': 1, 'bbox': [105, 264, 636, 356], 'text': '', 'parent': '', 'Model': 'YOLO', 'child': []}, {'uuid': 'eb52c3da-7094-4885-b15e-b2a44c447ec2', 'class': 'Label', 'confidence': 0.959921658039093, 'bbox': [647, 270, 799, 365], 'text': 'Branch', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '8d47773a-d265-422d-989c-27d0a7b7dcb7', 'class': 'Label', 'confidence': 0.8694050908088684, 'bbox': [1425, 283, 1601, 337], 'text': 'PAN No.', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': 'c2300e46-cfa0-4f35-8da3-24cdad856183', 'class': 'Label', 'confidence': 0.9539227485656738, 'bbox': [510, 406, 956, 454], 'text': ' NEFT/RTGS a sum', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '14e30edc-f277-4a63-9013-500260bbda4c', 'class': 'Label', 'confidence': 0.9764330387115479, 'bbox': [1386, 405, 1731, 459], 'text': 'Rupees in words)', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '49df9112-a65d-4eaa-a428-e2a1e234df0d', 'class': 'Label', 'confidence': 0.9706231355667114, 'bbox': [199, 580, 310, 629], 'text': 'Cash', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '24a7b668-6a00-4730-9ee8-c054f9577e5e', 'class': 'Label', 'confidence': 0.9664389491081238, 'bbox': [502, 577, 671, 632], 'text': 'Cheque', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': 'c697101c-4788-4037-a8b5-af73feb0f74e', 'class': 'Label', 'confidence': 0.9602909684181213, 'bbox': [849, 574, 1281, 639], 'text': 'Debit my / our account', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '84a0d450-9f6a-467b-a5da-7a6a70005075', 'class': 'Label', 'confidence': 0.9879698753356934, 'bbox': [111, 828, 364, 867], 'text': 'Account Number', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '47789110-49cb-4109-81be-be3860fab37e', 'class': 'Label', 'confidence': 0.9815024137496948, 'bbox': [101, 887, 364, 940], 'text': 'Cheque Number', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '5975c5c1-b4ee-43b1-9711-3f1e6c348ab3', 'class': 'Label', 'confidence': 0.9648439288139343, 'bbox': [1498, 914, 1698, 950], 'text': 'Cheque Date', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': 'bd19f4e4-68f5-455f-9a51-3df9cf688d61', 'class': 'Label', 'confidence': 0.9367125630378723, 'bbox': [105, 959, 371, 1009], 'text': \"Remitter's Name\", 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '6afae57c-9ea4-4f3a-905d-ab557f052103', 'class': 'Label', 'confidence': 0.988251268863678, 'bbox': [101, 1028, 247, 1083], 'text': 'Address', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': 'a3603db1-6ff9-4e34-bab9-a2f463dfea62', 'class': 'Label', 'confidence': 0.9915297031402588, 'bbox': [105, 1174, 403, 1217], 'text': 'Mobile/OtherNumber', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': 'd65d6163-8207-4bec-a855-d6f396fe98fa', 'class': 'Label', 'confidence': 0.9536399841308594, 'bbox': [101, 1309, 403, 1359], 'text': \"Beneticiary's Name\", 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '30e5a00f-7cdc-4285-80f4-852073cd76e8', 'class': 'Label', 'confidence': 0.9869080185890198, 'bbox': [105, 1385, 364, 1428], 'text': 'Account Number', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '272d0273-b0f1-43ed-be15-aaeb8f1dee69', 'class': 'Label', 'confidence': 0.9628899693489075, 'bbox': [101, 1447, 296, 1501], 'text': 'Bank Name', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '7265aa60-70e8-4b49-bc40-0f99f233a863', 'class': 'Label', 'confidence': 0.9369374513626099, 'bbox': [101, 1520, 364, 1574], 'text': 'fs code (11-digit)', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '296facc6-88fa-44fa-9389-8b38570bb3f5', 'class': 'Label', 'confidence': 0.9678309559822083, 'bbox': [101, 1589, 351, 1639], 'text': 'Branch Address', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': 'f0340c4b-994d-4ed8-9184-5678caa6dad3', 'class': 'Label', 'confidence': 0.9392633438110352, 'bbox': [102, 1672, 780, 1722], 'text': 'Sender to Receiver information (if any):', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '36d919c4-8468-4ead-8e2d-d180bd4e8ea1', 'class': 'Label', 'confidence': 0.9554519057273865, 'bbox': [789, 1860, 1104, 1918], 'text': 'Photo lD ', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '0e490d17-d6c3-45f3-846c-d1ebdf1bcbeb', 'class': 'Label', 'confidence': 0.9943037033081055, 'bbox': [87, 2378, 271, 2416], 'text': 'Signature-Primary Applicant', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '199f7849-bd7b-4e67-a354-007a41a79b0f', 'class': 'Label', 'confidence': 0.9973983764648438, 'bbox': [833, 2381, 987, 2417], 'text': 'Signature-Joint Applicant1', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '64d4a605-707d-43dc-8b23-c5b39d6724db', 'class': 'Label', 'confidence': 0.9963862895965576, 'bbox': [1554, 2384, 1708, 2421], 'text': 'Signature-Joint Applicant 2', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '660db3d8-f047-4f06-8c3e-099121e3bd31', 'class': 'Label', 'confidence': 0.9554936289787292, 'bbox': [121, 2668, 420, 2711], 'text': \"Beneticiary's Name\", 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '5e8b5e1e-2f80-4b11-a2d7-4e748851957f', 'class': 'Label', 'confidence': 0.9914286732673645, 'bbox': [1170, 2671, 1433, 2714], 'text': 'Account Number', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': 'c80ec557-4e34-4a2a-b50e-5c586732dce7', 'class': 'Label', 'confidence': 0.964317798614502, 'bbox': [659, 2744, 802, 2789], 'text': 'sum', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': 'd4426877-30ba-484b-8cac-be739e127c28', 'class': 'Label', 'confidence': 0.9403666257858276, 'bbox': [1161, 2747, 1439, 2790], 'text': '(Rupees in words', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': '8c3adbf4-74a5-428c-9a91-8879468621e4', 'class': 'Label', 'confidence': 0.9493101835250854, 'bbox': [913, 2910, 1121, 2962], 'text': 'Reference no', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'uuid': 'b1084cf9-84ca-4b97-b55a-4ab235947d94', 'class': 'Label', 'confidence': 0.9634560346603394, 'bbox': [115, 3054, 216, 3090], 'text': 'Date:', 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'id': '57012035-f800-4280-afc4-98723b01984f', 'text': 'Branch', 'confidence': 1, 'class': 'Label', 'bbox': [112, 2987, 220, 3032], 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'id': 'd3a2b2a5-7344-4f8b-a458-8a3499a8e293', 'text': 'Name and Signature', 'confidence': 1, 'class': 'Label', 'bbox': [704, 2977, 1213, 3048], 'child': [], 'Model': 'PADDLE', 'parent': ''}, {'id': 'c11b6727-c1f5-4af2-9ccb-54e3598c3b11', 'text': 'Date', 'confidence': 1, 'class': 'Label', 'bbox': [1598, 216, 1705, 278], 'child': [], 'Model': 'PADDLE', 'parent': ''}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Replace 'your_file.json' with the path to your JSON file\n",
    "with open('./Code/finalCode/static/Templates/NEFT/layout.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Now 'data' contains the JSON content as a Python dictionary or list\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a preprocessing function\n",
    "# def preprocess_image(image):\n",
    "#     # Convert to grayscale\n",
    "#     image = image.convert('L')\n",
    "    \n",
    "#     # Enhance the contrast\n",
    "#     enhancer = ImageEnhance.Contrast(image)\n",
    "#     image = enhancer.enhance(2)\n",
    "\n",
    "#     # Apply a sharpness filter to enhance edges\n",
    "#     image = image.filter(ImageFilter.SHARPEN)\n",
    "\n",
    "#     # Optionally, apply a binary thresholding to clean the background\n",
    "#     threshold = 128\n",
    "#     image = image.point(lambda p: p > threshold and 255)\n",
    "\n",
    "#     return image\n",
    "\n",
    "\n",
    "# def detect_text(cropped_Image):\n",
    "#     # i am think of implement where RCNN\n",
    "#     pass\n",
    "\n",
    "# @app.route('/process-empty-text-elements', methods=['POST'])\n",
    "# def process_empty_text_elements():\n",
    "#     try:\n",
    "#         # Get the layout data from the form (as JSON string)\n",
    "#         layout_data = request.form.get('layout')\n",
    "#         if layout_data:\n",
    "#             # Parse the JSON string into Python list or dict\n",
    "#             layout_data = json.loads(layout_data)  # Using json.loads for safety\n",
    "\n",
    "#         # Get the image file from the form\n",
    "#         image = request.files.get('image')\n",
    "        \n",
    "#         if image:\n",
    "#             # Save the image to the upload folder\n",
    "#             image_path = os.path.join(app.config['history'], image.filename)\n",
    "#             image.save(image_path)\n",
    "#             image = Image.open(image_path)\n",
    "#             image = preprocess_image(image)\n",
    "\n",
    "#         # Draw bounding boxes on the image\n",
    "#         draw = ImageDraw.Draw(image)\n",
    "#         for detection in layout_data:\n",
    "#             bbox = detection['bbox']\n",
    "#             draw.rectangle([bbox[0], bbox[1], bbox[2], bbox[3]], outline=\"red\", width=2)\n",
    "\n",
    "#         # Save the image with bounding boxes\n",
    "#         output_image_path = os.path.join(app.config['history'], 'output_with_bboxes.png')\n",
    "#         image.save(output_image_path)\n",
    "#         print(f\"Image with bounding boxes saved to {output_image_path}\")\n",
    "\n",
    "#         # Process each bounding box and recognize text using PaddleOCR\n",
    "#         updated_layout = []\n",
    "#         for detection in layout_data:\n",
    "#             bbox = detection['bbox']\n",
    "#             cropped_image = image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "#             recognized_text = detect_text(cropped_image)\n",
    "#             if recognized_text != \"\":\n",
    "#                 updated_layout.append(\n",
    "#                     {\n",
    "#                         \"uuid\":detection[\"uuid\"],\n",
    "#                         \"text\" :recognized_text\n",
    "#                     }\n",
    "#                 )\n",
    "#                 print(recognized_text)\n",
    "\n",
    "#         # Return a response back to the frontend\n",
    "#         return jsonify({\n",
    "#             \"message\": \"Data and image processed successfully\",\n",
    "#             \"status\": \"success\",\n",
    "#             \"updated_layout\": updated_layout\n",
    "#         }), 200\n",
    "\n",
    "#     except Exception as e:\n",
    "#         # Handle any errors that occur\n",
    "#         print(e)\n",
    "#         return jsonify({\"message\": str(e), \"status\": \"error\"}), 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = './Code/finalCode/Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "TrOCR_processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "TrOCR_processor.save_pretrained(\"./Code/finalCode/Models/TrOCR-processor/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "TrOCR_model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "TrOCR_model.save_pretrained(\"./Code/finalCode/Models/TrOCR-model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# @app.route('/process-empty-text-elements', methods=['POST'])\n",
    "# def process_empty_text_elements():\n",
    "#     try:\n",
    "#         # Get the layout data from the form (as JSON string)\n",
    "#         layout_data = request.form.get('layout')\n",
    "#         if layout_data:\n",
    "#             # Parse the JSON string into Python list or dict\n",
    "#             layout_data = eval(layout_data)  # or use json.loads(layout_data) if it's a string\n",
    "#             # print('Received layout data:', layout_data)\n",
    "\n",
    "#         # Get the image file from the form\n",
    "#         image = request.files.get('image')\n",
    "#         if image:\n",
    "#             # Save the image to the upload folder\n",
    "#             image_path = os.path.join(app.config['history'], image.filename)\n",
    "#             image.save(image_path)\n",
    "#             image = PIL.Image.open(image)\n",
    "\n",
    "#             # Check if the image has an alpha channel (RGBA), and convert it to RGB\n",
    "#             if image.mode == 'RGBA':\n",
    "#                 image = image.convert('RGB')\n",
    "#             print('Converted image from RGBA to RGB')\n",
    "\n",
    "#             print(f\"Image saved to {image_path}\")\n",
    "\n",
    "#         # Draw bounding boxes on the image\n",
    "#         draw = ImageDraw.Draw(image)\n",
    "#         for detection in layout_data:\n",
    "#             bbox = detection['bbox']\n",
    "#             draw.rectangle([bbox[0], bbox[1], bbox[2], bbox[3]], outline=\"red\", width=2)\n",
    "\n",
    "#         # Save the image with bounding boxes\n",
    "#         output_image_path = os.path.join(app.config['history'], 'output_with_bboxes.png')  # Save as PNG or your desired format\n",
    "#         image.save(output_image_path)\n",
    "#         print(f\"Image with bounding boxes saved to {output_image_path}\")\n",
    "\n",
    "#         # Process each bounding box and recognize text using TrOCR\n",
    "#         updated_layout = []\n",
    "#         for detection in layout_data:\n",
    "#             bbox = detection['bbox']\n",
    "#             print(bbox)\n",
    "\n",
    "#             # Crop the image according to the bounding box (bbox = [x1, y1, x2, y2])\n",
    "#             cropped_image = image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "#             print(type(cropped_image))\n",
    "\n",
    "#             # Convert the cropped image to the format required by the model (RGB image)\n",
    "#             pixel_values = TrOCR_processor(images=cropped_image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "#             # Perform text recognition using TrOCR\n",
    "#             generated_ids = TrOCR_model.generate(pixel_values)\n",
    "#             recognized_text = TrOCR_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "#             # Update the detection with the recognized text\n",
    "#             detection['text'] = recognized_text.strip()  # Update the text field with recognized text\n",
    "#             updated_layout.append(detection)\n",
    "#             print(detection, \"\\n\")\n",
    "\n",
    "#         # Return a response back to the frontend\n",
    "#         return jsonify({\"message\": \"Data and image received successfully\", \"status\": \"success\", \"updated_layout\": updated_layout}), 200\n",
    "\n",
    "#     except Exception as e:\n",
    "#         # Handle any errors that occur\n",
    "#         print(e)\n",
    "#         return jsonify({\"message\": str(e), \"status\": \"error\"}), 400    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
